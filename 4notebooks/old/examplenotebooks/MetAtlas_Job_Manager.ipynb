{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parse the fileInfo sheet and return the fileIDs and sample grouping values\n",
    "#     export_fileIds,myArray,fileInfo,filename = metatlas_func.get_FileInfo(myFileInfo)\n",
    "\n",
    "#Add the group names to each file at NERSC\n",
    "#     metatlas_func.addGroupInfoToFiles(client,fileInfo)\n",
    "\n",
    "#get the specification for each compound\n",
    "#     dictData = metatlas_func.getAtlasData(client,dictId)\n",
    "\n",
    "#get the EIC, for each compound (in reality, it should get peak summary, MSMS, EIC, and Spectrum)\n",
    "#     data = metatlas_func.getData(export_fileIds,dictData,myArray,client,polarity,extraTime)\n",
    "\n",
    "#get the total intensity chromatogram for each file\n",
    "#     ticData = metatlas_func.getAllTICS(export_fileIds,polarity,myArray,client)\n",
    "\n",
    "#pickle everything in case NERSC shuts down\n",
    "#     metatlas_func.saveData(myFolder,ticData,dictId,myExperimentID,dictData,data,fileInfo,export_fileIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import getpass\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import groupby\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "import codecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "client = requests.Session()\n",
    "client = metatlas_func.authenticateUser(client,'bpb')\n",
    "if len(client.cookies)>1:\n",
    "    print \"Successfully authenticated\"\n",
    "else:\n",
    "    print \"Login failed.  Either try again or contact NERSC to reset your password.  Use http://nim.nersc.gov to manage user account settings.\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def authenticateUser(client,username):\n",
    "    password = getpass.getpass()\n",
    "    authURL = 'https://metatlas.nersc.gov/client/login/'\n",
    "    # Retrieve the CSRF token first\n",
    "    client.get(authURL) # sets cookie\n",
    "    csrftoken = client.cookies['csrftoken']\n",
    "    login_data = dict(username=username, password=password, csrfmiddlewaretoken=csrftoken, next='/')\n",
    "    r = client.post(authURL, data=login_data, headers=dict(Referer=authURL))\n",
    "    return client\n",
    "\n",
    "def shareExperiment(client,username,myExperimentID):\n",
    "    # # Share the experiment and dictionary with another user.\n",
    "    payload = {\"user\":username,\"perms\":[\"read\",\"write\"]}\n",
    "    sendData=json.dumps(payload)\n",
    "    url = 'https://metatlas.nersc.gov/api/experiment/%s/share/' % myExperimentID\n",
    "    r = client.post(url, data=sendData)\n",
    "    # # print r.content\n",
    "\n",
    "def shareAtlas(client,username,dictId):\n",
    "    payload = {\"user\":username,\"perms\":[\"read\",\"write\"]}\n",
    "    sendData=json.dumps(payload)\n",
    "    url = 'https://metatlas.nersc.gov/api/dict/%s/share/' % dictId\n",
    "    r = client.post(url, data=sendData)\n",
    "\n",
    "def makeFileInfoSheet(client,myExperimentID,finfo_filename):\n",
    "    url = 'https://metatlas.nersc.gov/api/experiment/%s' % myExperimentID\n",
    "    r = client.get(url)\n",
    "    files = json.loads(r.content)\n",
    "    fileInfo = {'fid':[],'name':[],'status':[]};\n",
    "    fid = open(finfo_filename,'wb')\n",
    "    fid.write('index\\tstatus\\tname\\tfid\\tpolarity\\tgroup\\tinclusion_order\\tnormalization_factor\\tretention_correction\\n')\n",
    "    for i,myRun in enumerate(files[u'runs']):\n",
    "        splitPathToFile = os.path.split(myRun[u'in_file'])\n",
    "        fid.write('%d\\t%d\\t%s\\t%d\\tpos\\tgroup1\\n' % (i,myRun[u'pending'],splitPathToFile[1],myRun[u'_id'][u'file_id']))\n",
    "        if myRun[u'pending'] == 0:\n",
    "            fileInfo['fid'].append(myRun[u'_id'][u'file_id'])\n",
    "            fileInfo['name'].append(splitPathToFile[1])\n",
    "            fileInfo['status'].append(myRun[u'pending']) #only keep if status is 0\n",
    "    pathYouWant = splitPathToFile[0] # TODO: we will have to see what this will do on a window's computer.  taking a linux path and using os.\n",
    "    fid.close()\n",
    "\n",
    "def exportAtlas(client,atlasID,filename):\n",
    "    url = 'https://metatlas.nersc.gov/api/dict/%s/' % atlasID\n",
    "    r = client.get(url)\n",
    "    dictData = json.loads(r.text)\n",
    "    # export an atlas\n",
    "    myList = ['name','pubchem_id','formula','neutral_mass','mz','mz_threshold','adducts','rt_max','rt_min','rt_peak']\n",
    "    import csv\n",
    "    fid = open(filename,'wb')\n",
    "    for listItem in myList:\n",
    "        fid.write('%s\\t' % listItem)\n",
    "    fid.write('\\n')\n",
    "    for i,compound in enumerate(dictData[u'compounds']):\n",
    "        for listItem in myList:\n",
    "            if listItem == 'name':\n",
    "                fid.write('%s\\t' % compound[listItem].encode('utf-8'))\n",
    "            else:\n",
    "                fid.write('%s\\t' % compound[listItem])\n",
    "        fid.write('\\n')\n",
    "    fid.close()\n",
    "\n",
    "# import the fileInfo sheet with annotated group information, polarity, and plot order\n",
    "def get_FileInfo(myFileInfo):\n",
    "    myArray = 'lcms_test_1' #files[u'runs'][0][u'_id'][u'array_name']\n",
    "    filename = '%s' % (myFileInfo)\n",
    "    with open(filename,'rU') as file_object:\n",
    "        newfileInfo = list(csv.DictReader(file_object, dialect='excel-tab'))\n",
    "    keys = newfileInfo[0].iterkeys()\n",
    "    fileInfo = {key: [d[key] for d in newfileInfo] for key in keys}\n",
    "    fileInfo['fid'] = map(int, fileInfo['fid'])\n",
    "    fileInfo['index'] = map(int, fileInfo['index'])\n",
    "    fileInfo['inclusion_order'] = map(int, fileInfo['inclusion_order'])\n",
    "    fileInfo['status'] = map(int, fileInfo['status'])\n",
    "    fileInfo['normalization_factor'] = map(float, fileInfo['normalization_factor'])\n",
    "    fileInfo['retention_correction'] = map(float, fileInfo['retention_correction'])\n",
    "\n",
    "    idx = np.argsort(fileInfo['inclusion_order'])\n",
    "    export_fileIds = np.asarray(fileInfo['fid'])[idx]\n",
    "    return export_fileIds,myArray,fileInfo,filename\n",
    "\n",
    "def addGroupInfoToFiles(client,fileInfo):\n",
    "    for i,f in enumerate(fileInfo['fid']):\n",
    "        url = 'https://metatlas.nersc.gov/api/metadata/lcms_test_1/%d/' % f\n",
    "        r = client.patch(url, data=json.dumps({\"sample_type\": fileInfo['group'][i]}))\n",
    "\n",
    "def createNewAtlas(client,atlasName,sampleDescription,methodDescription):\n",
    "    # {\"name\":\"\",\"sample\":\"\",\"method\":\"\"}\n",
    "    payload = {\"name\":atlasName,\"sample\":sampleDescription,\"method\":methodDescription}\n",
    "    sendData=json.dumps(payload)\n",
    "    client.headers.update({'referer': 'https://metatlas.nersc.gov/dict/create'})\n",
    "    url = 'https://metatlas.nersc.gov/api/dict/'\n",
    "    r = client.post(url, data=sendData)\n",
    "    retData = json.loads(r.text)\n",
    "    return retData['id']\n",
    "\n",
    "\n",
    "# curl 'https://metatlas.nersc.gov/api/dict/' -H 'Cookie: __utma=25799074.440104800.1398372604.1436306658.1436966664.89; __utmc=25799074; __utmz=25799074.1436966664.89.62.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=(not%20provided); _ga=GA1.2.440104800.1398372604; __utmt=1; csrftoken=4WbUyL4Sg6GH5H11w7ggPHnp1Poh3VGq; sessionid=i8s8b49woppom27dy8u0c9fhwyom6n15; __utma=250901914.440104800.1398372604.1437097534.1437176628.217; __utmb=250901914.4.9.1437176632539; __utmc=250901914; __utmz=250901914.1434753096.197.3.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=(not%20provided)' -H 'Origin: https://metatlas.nersc.gov' -H 'Accept-Encoding: gzip, deflate' -H 'Accept-Language: en-US,en;q=0.8' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36' -H 'Content-Type: application/x-www-form-urlencoded; charset=UTF-8' -H 'Accept: */*' -H 'Referer: https://metatlas.nersc.gov/dict/create/' -H 'X-Requested-With: XMLHttpRequest' -H 'Connection: keep-alive' --data '{\"name\":\"\",\"sample\":\"\",\"method\":\"\"}' --compressed\n",
    "\n",
    "def addFromSpreadsheetToAtlas(client,filename,dictId):\n",
    "    #add compounds to an atlas from a well formated spreadsheet that are new and\n",
    "    #update any that were already there, but have changed\n",
    "\n",
    "    with open(filename,'rU') as file_object:\n",
    "        sheetData = list(csv.DictReader(file_object, dialect='excel-tab'))\n",
    "    url = 'https://metatlas.nersc.gov/api/dict/%s/' % dictId\n",
    "    r = client.get(url)\n",
    "    dictData = json.loads(r.text)\n",
    "\n",
    "    for compound in sheetData:\n",
    "    #     print compound\n",
    "        compound['name'] = re.sub(\"\\xca\",'',compound['name'])\n",
    "        if compound['name'][0] == ' ':\n",
    "            compound['name'] = compound['name'][1:]\n",
    "        cID = filter( lambda x: x[u'name']==compound['name'], dictData[u'compounds'])\n",
    "    #     print cID\n",
    "        if not cID:\n",
    "            # a new entry is created if that compound name doesn't exist\n",
    "            url = 'https://metatlas.nersc.gov/api/dict/%s/' % dictId\n",
    "    #                 url = 'https://metatlas.nersc.gov/api/compound/%s/' % cID[0][u'_id']\n",
    "            r = client.post(url, data=json.dumps([compound]))\n",
    "            print(r.text)\n",
    "        else:\n",
    "            # edit the entry if it already exists\n",
    "            url = 'https://metatlas.nersc.gov/api/compound/%s/' % cID[0][u'_id']\n",
    "    #         url = 'https://metatlas.nersc.gov/api/dict/%s/' % dictId\n",
    "            r = client.patch(url, data=json.dumps(compound))\n",
    "            print(r.text)\n",
    "\n",
    "def getAtlasData(client,dictId):\n",
    "    url = 'https://metatlas.nersc.gov/api/dict/%s/' % dictId\n",
    "    r = client.get(url)\n",
    "    dictData = json.loads(r.text)\n",
    "    return dictData\n",
    "\n",
    "def getExperimentData(client,myExperimentID):\n",
    "    url = 'https://metatlas.nersc.gov/api/experiment/%s' % myExperimentID\n",
    "    r = client.get(url)\n",
    "    return json.loads(r.content)\n",
    "\n",
    "def getEICForCompounds(compound,myArray,files_I_want,rtTol,client,polarity):\n",
    "    if isinstance(files_I_want,int):\n",
    "        myList = str(files_I_want)\n",
    "    else:\n",
    "        myList = ','.join(map(str, files_I_want))\n",
    "\n",
    "\n",
    "\n",
    "    mz = float(compound[u'mz'])\n",
    "    mzTol = float(compound[u'mz_threshold'])\n",
    "    mzMin = mz - mz*mzTol/1.0e6\n",
    "    mzMax = mz + mz*mzTol/1.0e6\n",
    "    rtMin = float(compound[u'rt_min'])-rtTol\n",
    "    rtMax = float(compound[u'rt_max'])+rtTol\n",
    "    rtPeak = float(compound[u'rt_peak'])\n",
    "\n",
    "    payload = {'L':1,'P':polarity,'arrayname':myArray,'fileidlist':myList,\n",
    "              'max_mz':mzMax,'min_mz':mzMin,\n",
    "              'min_rt':rtMin,'max_rt':rtMax,\n",
    "              'nsteps':20000,'queryType':'XICofFile_mf'}\n",
    "    url = 'https://metatlas.nersc.gov/api/run'\n",
    "    r = client.get(url,params=payload)\n",
    "    if r.content:\n",
    "        data = np.asarray(json.loads(r.content))\n",
    "        return data\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def getData(export_fileIds,dictData,myArray,client,polarity,extraTime):\n",
    "    import time\n",
    "    data = []\n",
    "    for i,compound in enumerate(dictData[u'compounds']):\n",
    "        print i, compound['name']\n",
    "        data.append(getEICForCompounds(compound,myArray,export_fileIds,extraTime,client,polarity))\n",
    "        time.sleep(4) \n",
    "    return data\n",
    "\n",
    "def getAllTICS(export_fileIds,polarity,myArray,client):\n",
    "    # prototype function to get TIC for all runs\n",
    "    # get TIC for all runs\n",
    "    if isinstance(export_fileIds,int):\n",
    "        myList = str(export_fileIds)\n",
    "    else:\n",
    "        myList = ','.join(map(str, export_fileIds))\n",
    "\n",
    "    payload = {'L':1,'P':polarity,'arrayname':myArray,'fileidlist':myList,\n",
    "              'max_mz':2000,'min_mz':100,\n",
    "              'min_rt':1,'max_rt':300,\n",
    "              'nsteps':20000,'queryType':'XICofFile_mf'}\n",
    "    url = 'https://metatlas.nersc.gov/api/run'\n",
    "    r = client.get(url,params=payload)\n",
    "    ticData = np.asarray(json.loads(r.content))\n",
    "    return ticData\n",
    "\n",
    "def clusterRTCentroids(rt,cutoff):\n",
    "    rt = np.asarray(rt)\n",
    "    dists = np.abs(rt - rt[:, None])\n",
    "    Y = linkage(dists, method='single', metric='euclidean')\n",
    "    C = fcluster(Y,cutoff)\n",
    "    return C\n",
    "\n",
    "def makeDataMat(export_fileIds,data,fileInfo,dictData):\n",
    "    # Build the datamat.  \n",
    "    # Each row is a metabolite.  \n",
    "    # Each column is a run that is indexed by its filename and group.\n",
    "    dataMat = np.zeros((len(data),len(export_fileIds)))\n",
    "    rtMat = np.zeros((len(data),len(export_fileIds)))\n",
    "    rowLabels = []\n",
    "    colLabels = []\n",
    "    rowGroups = []\n",
    "    colGroups = []\n",
    "\n",
    "    rtCorr = []\n",
    "    for f in export_fileIds:\n",
    "        for i,f2 in enumerate(fileInfo['fid']):\n",
    "            if f2 == f:\n",
    "                rtCorr.append(fileInfo['retention_correction'][i])\n",
    "    # do it once for all files (used below in clustergram)\n",
    "    for i,myFile in enumerate(export_fileIds):\n",
    "        for j,fid in enumerate(fileInfo['fid']):\n",
    "            if fid == myFile:\n",
    "                colLabels.append(fileInfo['name'][j])\n",
    "                colGroups.append(fileInfo['group'][j])\n",
    "                \n",
    "    for i,datum in enumerate(data):\n",
    "        rowLabels.append(dictData[u'compounds'][i]['name'])\n",
    "        mz = float(dictData[u'compounds'][i][u'mz'])\n",
    "        mzTol = float(dictData[u'compounds'][i][u'mz_threshold'])\n",
    "        mzMin = mz - mz*mzTol/1.0e6\n",
    "        mzMax = mz + mz*mzTol/1.0e6\n",
    "        rtMin = float(dictData[u'compounds'][i][u'rt_min'])\n",
    "        rtMax = float(dictData[u'compounds'][i][u'rt_max'])\n",
    "        rowGroups.append('Metabolite')\n",
    "        for j,myFile in enumerate(export_fileIds):\n",
    "            if datum.size>3:\n",
    "\n",
    "                idx = np.logical_and( datum[:,2]==myFile, datum[:,0]>=(rtMin+float(rtCorr[j])), datum[:,0]<=(rtMax+float(rtCorr[j])) )\n",
    "                if np.sum(idx)>0:\n",
    "                    x1 = datum[:,0][idx]\n",
    "                    y1 = datum[:,1][idx]\n",
    "                    # y1 = y1 - np.min(y1)\n",
    "    #                 y1 = y1[:] / fileInfo['normalization_factor'][j]\n",
    "                    dataMat[i,j] = np.sum(y1)\n",
    "                    if dataMat[i,j] > 0:\n",
    "                        rtMat[i,j] = np.sum(np.multiply(x1,y1)) / np.sum(y1)\n",
    "    return rowLabels,rowGroups,colLabels,colGroups,dataMat,rtMat\n",
    "\n",
    "def calcGroupVals(colGroups,rowLabels,dataMat):\n",
    "    # From Datamat Build the mat of means and std for the groups\n",
    "    uGroups = np.unique(colGroups)\n",
    "    meanMat = np.zeros((len(rowLabels),len(uGroups)))\n",
    "    stdevMat = np.zeros((len(rowLabels),len(uGroups)))\n",
    "    cvMat = np.zeros((len(rowLabels),len(uGroups)))\n",
    "    stderrMat = np.zeros((len(rowLabels),len(uGroups)))\n",
    "    numinMat = np.zeros((len(rowLabels),len(uGroups)))\n",
    "\n",
    "    for i,met in enumerate(rowLabels):\n",
    "        for j,gro in enumerate(uGroups):\n",
    "            idx = [ii for ii, jj in enumerate(colGroups) if jj == gro]\n",
    "            meanMat[i,j] = np.mean(dataMat[i,idx])\n",
    "            stdevMat[i,j] = np.std(dataMat[i,idx])\n",
    "            stderrMat[i,j] = np.std(dataMat[i,idx]) / len(idx)**0.5\n",
    "            numinMat[i,j] = len(idx)\n",
    "            if meanMat[i,j] > 0:\n",
    "                cvMat[i,j] = stdevMat[i,j] / meanMat[i,j]\n",
    "    return uGroups,meanMat,stdevMat,cvMat,stderrMat,numinMat\n",
    "\n",
    "def exportCompoundAreas(myLabelString,uGroups,rowLabels,colLabels,dataMat,meanMat,rtMat,stdevMat,cvMat,stderrMat,numinMat,dictData):\n",
    "    output_filename = 'data/%s/peakHeight_Table_%s.tab' % (myLabelString,myLabelString) #re.sub('fileInfo','peakArea_Table_',re.sub('txt','tab',filename))\n",
    "    export_filenames = []\n",
    "    myList = ['name','pubchem_id','formula','neutral_mass','mz','mz_threshold','adducts','rt_max','rt_min','rt_peak']\n",
    "\n",
    "    with codecs.open(output_filename, 'w', encoding='utf-8') as fid:  \n",
    "        for listItem in myList:\n",
    "            fid.write('%s\\t' % listItem)\n",
    "\n",
    "        for j,gro in enumerate(uGroups):\n",
    "            fid.write('Mean %s\\t' % gro)\n",
    "\n",
    "        for j,gro in enumerate(uGroups):\n",
    "            fid.write('STDEV %s\\t' % gro)\n",
    "\n",
    "        for j,gro in enumerate(uGroups):\n",
    "            fid.write('STDERR %s\\t' % gro)\n",
    "\n",
    "        for j,gro in enumerate(uGroups):\n",
    "            fid.write('CV %s\\t' % gro)\n",
    "            \n",
    "        for j,gro in enumerate(uGroups):\n",
    "            fid.write('Num in %s\\t' % gro)\n",
    "\n",
    "        for filename in colLabels:\n",
    "            fid.write('%s Peak Area\\t' % filename)\n",
    "\n",
    "        for filename in colLabels:\n",
    "            fid.write('%s Retention Time in\\t' % filename)\n",
    "\n",
    "        fid.write('\\n')\n",
    "\n",
    "        for i,met in enumerate(rowLabels):\n",
    "            compound = dictData[u'compounds'][i]\n",
    "            for listItem in myList:\n",
    "                fid.write('%s\\t' % compound[listItem])\n",
    "\n",
    "            for j,gro in enumerate(uGroups):\n",
    "                fid.write('%5.2f\\t' % meanMat[i,j])\n",
    "            for j,gro in enumerate(uGroups):\n",
    "                fid.write('%5.2f\\t' % stdevMat[i,j])\n",
    "            for j,gro in enumerate(uGroups):\n",
    "                fid.write('%5.2f\\t' % stderrMat[i,j])\n",
    "            for j,gro in enumerate(uGroups):\n",
    "                fid.write('%5.2f\\t' % cvMat[i,j])\n",
    "            for j,gro in enumerate(uGroups):\n",
    "                fid.write('%5.2f\\t' % numinMat[i,j])\n",
    "                \n",
    "            for j,f in enumerate(colLabels):\n",
    "                fid.write('%5.2f\\t' % dataMat[i,j])\n",
    "            for j,f in enumerate(colLabels):\n",
    "                fid.write('%5.2f\\t' % rtMat[i,j])\n",
    "            fid.write('\\n')\n",
    "\n",
    "def plotTICs(colLabels,export_fileIds,ticData,myLabelString):\n",
    "    # Plot All the Tics\n",
    "    for i,f in enumerate(colLabels):\n",
    "        fig = plt.figure(1, figsize=(18, 8))\n",
    "        ax = fig.add_subplot(111)\n",
    "        x = ticData[ticData[:,2]==export_fileIds[i],0]\n",
    "        y = ticData[ticData[:,2]==export_fileIds[i],1]\n",
    "        idx = np.argsort(x)\n",
    "        ax.plot(x[idx],y[idx])\n",
    "        ax.set_xlabel('Time (min)')\n",
    "        ax.set_ylabel('Magnitude (TIC)')\n",
    "        fname = 'data/%s/TICs/TIC_%s %s%s' % (myLabelString,myLabelString,re.sub('[^A-Za-z0-9]+', '_', f),'.pdf')\n",
    "        fig.savefig(fname)\n",
    "        fig.clear()\n",
    "\n",
    "def plotChromatograms(rowLabels,myLabelString,export_fileIds,fileInfo,dictData,numCols,data,equalaxis):\n",
    "    # for each compound, make a chromatogram for each file\n",
    "    # each filename is going to be a compound name\n",
    "    plt.rcParams['pdf.fonttype']=42\n",
    "    # plt.rcParams['pdf.useafm'] = True\n",
    "    plt.rcParams['pdf.use14corefonts'] = True\n",
    "    plt.rcParams['text.usetex'] = False\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    plt.rcParams.update({'font.weight': 'bold'})\n",
    "    plt.rcParams['axes.linewidth'] = 2 # set the value globally\n",
    "\n",
    "    export_filenames = []\n",
    "    for i,cname in enumerate(rowLabels):\n",
    "        export_filenames.append('data/%s/Chromatograms/Chromatograms_%s %s%s' % (myLabelString,myLabelString,re.sub('[^A-Za-z0-9]+', '_', cname),'.pdf'))\n",
    "\n",
    "    subplot_titles = []\n",
    "    for i,myFile in enumerate(export_fileIds):\n",
    "        for j,fid in enumerate(fileInfo['fid']):\n",
    "            if fid == myFile:\n",
    "                subplot_titles.append(fileInfo['name'][j].replace('.mzML',''))\n",
    "\n",
    "    # %config InlineBackend.figure_format = 'png'\n",
    "    rtCorr = []\n",
    "    for f in export_fileIds:\n",
    "        for i,f2 in enumerate(fileInfo['fid']):\n",
    "            if f2 == f:\n",
    "                rtCorr.append(fileInfo['retention_correction'][i])\n",
    "    \n",
    "    nRows = int(np.ceil(len(export_fileIds)/numCols))\n",
    "\n",
    "    for i,compound in enumerate(dictData[u'compounds']):\n",
    "        fig, ax = plt.subplots(nRows, int(numCols),figsize=(8*numCols,nRows * 6))\n",
    "        min_x_val = 1000000\n",
    "        max_x_val = 0\n",
    "        max_y_val = 0\n",
    "        myname = dictData[u'compounds'][i]['name']\n",
    "        for j,a in enumerate(ax.flat):\n",
    "            a.plot(float(compound[u'rt_peak'])+rtCorr[j],1e12,'.')\n",
    "            a.axvline(float(compound[u'rt_min'])+rtCorr[j],linewidth=2, color='k') #original rtMin\n",
    "            a.axvline(float(compound[u'rt_max'])+rtCorr[j],linewidth=2, color='k') #original rtMax\n",
    "            a.axvline(float(compound[u'rt_peak'])+rtCorr[j],linewidth=2, color='r') #original rtPeak\n",
    "            a.set_xlabel('Time (min)',weight='bold')\n",
    "            a.set_ylabel('Intensity (au)',weight='bold')\n",
    "            a.set_title(subplot_titles[j],fontsize=12,weight='bold')\n",
    "            if j<len(export_fileIds):\n",
    "                if len(data[i])>3:\n",
    "                    x1 = data[i][:,0][(data[i][:,2]==export_fileIds[j])]\n",
    "                    y1 = data[i][:,1][(data[i][:,2]==export_fileIds[j])]\n",
    "                    if x1.size>0:\n",
    "    #                     if myname.startswith('IST'):\n",
    "    #                         y1 = y1[:]\n",
    "    #                     else:\n",
    "    #                         y1 = y1[:] / fileInfo['normalization_factor'][j]\n",
    "                        idx = np.argsort(x1)\n",
    "                        x1 = x1[idx]\n",
    "                        y1 = y1[idx]\n",
    "                        y1 = y1 - np.min(y1)\n",
    "                        a.plot(x1,y1,'k-',linewidth=2.0,alpha=1.0)\n",
    "                        \n",
    "                        myWhere = np.logical_and(x1>=(float(compound[u'rt_min'])+float(rtCorr[j])), x1<=(float(compound[u'rt_max'])+float(rtCorr[j])) )\n",
    "\n",
    "                        a.fill_between(x1,0,y1,myWhere, facecolor='c', alpha=0.3) #new rtBounds\n",
    "                        \n",
    "\n",
    "                        \n",
    "                        if np.min(data[i][:,0])<min_x_val:\n",
    "                            min_x_val = np.min(data[i][:,0])\n",
    "                        if np.max(data[i][:,0])>max_x_val:\n",
    "                            max_x_val = np.max(data[i][:,0])\n",
    "                        if np.max(y1)>max_y_val:\n",
    "                            max_y_val = np.max(y1)\n",
    "        for j,a in enumerate(ax.flat):\n",
    "            a.set_xlim([min_x_val,max_x_val])\n",
    "        if equalaxis == 1:\n",
    "            for j,a in enumerate(ax.flat):\n",
    "                a.set_ylim([0,max_y_val])\n",
    "        fig.tight_layout()        \n",
    "        fig.savefig(export_filenames[i])\n",
    "        fig.clear()\n",
    "        plt.close('all')\n",
    "\n",
    "def plotBoxPlots(myLabelString,rowLabels,colGroups,dataMat,scale,fontsize):\n",
    "    plt.rcParams['pdf.fonttype']=42\n",
    "    # plt.rcParams['pdf.useafm'] = True\n",
    "    plt.rcParams['pdf.use14corefonts'] = True\n",
    "    plt.rcParams['text.usetex'] = False\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.rcParams.update({'font.weight': 'bold'})\n",
    "    plt.rcParams['axes.linewidth'] = 2 # set the value globally\n",
    "    for i,cname in enumerate(rowLabels):\n",
    "        myVals = []\n",
    "        for j,mygroup in enumerate(colGroups):\n",
    "            myVals.append((mygroup, dataMat[i,j]+1))\n",
    "        myVals = sorted(myVals, key=lambda x: x[0]) \n",
    "        data_to_plot = []\n",
    "        groupName = []\n",
    "        for key, group in groupby(myVals, lambda x: x[0]):\n",
    "            L = list(zip(*group)[1])\n",
    "            data_to_plot.append(L)\n",
    "            groupName.append(key)\n",
    "\n",
    "        fig = plt.figure(1, figsize=(18, 18))\n",
    "        ax = fig.add_subplot(111,yscale=scale)\n",
    "        bp = ax.boxplot(data_to_plot)\n",
    "        ax.set_xticklabels(groupName,rotation=40, ha='right',fontsize=fontsize)\n",
    "        ax.set_title(cname)\n",
    "        ax.grid('on',alpha=0.5)\n",
    "        ax.set_ylabel('Peak Area')\n",
    "        plt.tight_layout()\n",
    "        fig.savefig('data/%s/Boxplots/Boxplot_%s %s%s' % (myLabelString,myLabelString,re.sub('[^A-Za-z0-9]+', '', cname),'.pdf'))\n",
    "        fig.clear()\n",
    "    # plt.rcParams['ps.fonttype']=42\n",
    "    # plt.rcParams['ps.useafm']= True\n",
    "\n",
    "\n",
    "def plotChromatograms(rowLabels,myLabelString,export_fileIds,fileInfo,dictData,numCols,data,equalaxis):\n",
    "    # for each compound, make a chromatogram for each file\n",
    "    # each filename is going to be a compound name\n",
    "    plt.rcParams['pdf.fonttype']=42\n",
    "    # plt.rcParams['pdf.useafm'] = True\n",
    "    plt.rcParams['pdf.use14corefonts'] = True\n",
    "    plt.rcParams['text.usetex'] = False\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    plt.rcParams.update({'font.weight': 'bold'})\n",
    "    plt.rcParams['axes.linewidth'] = 2 # set the value globally\n",
    "\n",
    "    export_filenames = []\n",
    "    for i,cname in enumerate(rowLabels):\n",
    "        export_filenames.append('data/%s/Chromatograms/Chromatograms_%s %s%s' % (myLabelString,myLabelString,re.sub('[^A-Za-z0-9]+', '_', cname),'.pdf'))\n",
    "\n",
    "    subplot_titles = []\n",
    "    for i,myFile in enumerate(export_fileIds):\n",
    "        for j,fid in enumerate(fileInfo['fid']):\n",
    "            if fid == myFile:\n",
    "                subplot_titles.append(fileInfo['name'][j].replace('.mzML',''))\n",
    "\n",
    "    # %config InlineBackend.figure_format = 'png' \n",
    "    \n",
    "    nRows = int(np.ceil(len(export_fileIds)/numCols))\n",
    "\n",
    "    for i,compound in enumerate(dictData[u'compounds']):\n",
    "        fig, ax = plt.subplots(nRows, int(numCols),figsize=(8*numCols,nRows * 6))\n",
    "        min_x_val = 1000000\n",
    "        max_x_val = 0\n",
    "        max_y_val = 0\n",
    "        myname = dictData[u'compounds'][i]['name']\n",
    "        for j,a in enumerate(ax.flat):\n",
    "            if j<len(export_fileIds):\n",
    "                a.set_xlabel('Time (min)',weight='bold')\n",
    "                a.set_ylabel('Intensity (au)',weight='bold')\n",
    "                a.set_title(subplot_titles[j],fontsize=12,weight='bold')\n",
    "                a.axvline(float(compound[u'rt_min']),linewidth=2, color='k') #original rtMin\n",
    "                a.axvline(float(compound[u'rt_max']),linewidth=2, color='k') #original rtMax\n",
    "                a.axvline(float(compound[u'rt_peak']),linewidth=2, color='r') #original rtPeak\n",
    "                if len(data[i])>3:\n",
    "                    x1 = data[i][:,0][(data[i][:,2]==export_fileIds[j])]\n",
    "                    y1 = data[i][:,1][(data[i][:,2]==export_fileIds[j])]\n",
    "                    if x1.size>0:\n",
    "    #                     if myname.startswith('IST'):\n",
    "    #                         y1 = y1[:]\n",
    "    #                     else:\n",
    "    #                         y1 = y1[:] / fileInfo['normalization_factor'][j]\n",
    "                        idx = np.argsort(x1)\n",
    "                        x1 = x1[idx]\n",
    "                        y1 = y1[idx]\n",
    "                        y1 = y1 - np.min(y1)\n",
    "                        a.plot(x1,y1,'k-',linewidth=2.0,alpha=1.0)\n",
    "\n",
    "                        myWhere = np.logical_and((x1>=float(compound[u'rt_min'])), (x1<=float(compound[u'rt_max'])) )\n",
    "\n",
    "                        a.fill_between(x1,0,y1,myWhere, facecolor='c', alpha=0.3) #new rtBounds\n",
    "                        \n",
    "                        if np.min(data[i][:,0])<min_x_val:\n",
    "                            min_x_val = np.min(data[i][:,0])\n",
    "                        if np.max(data[i][:,0])>max_x_val:\n",
    "                            max_x_val = np.max(data[i][:,0])\n",
    "                        if np.max(y1)>max_y_val:\n",
    "                            max_y_val = np.max(y1)\n",
    "        for j,a in enumerate(ax.flat):\n",
    "            a.set_xlim([min_x_val,max_x_val])\n",
    "        if equalaxis == 1:\n",
    "            for j,a in enumerate(ax.flat):\n",
    "                a.set_ylim([0,max_y_val])\n",
    "        fig.tight_layout() \n",
    "        print export_filenames[i] \n",
    "        fig.savefig(export_filenames[i])\n",
    "        fig.clear()\n",
    "        plt.close('all')\n",
    "\n",
    "def plotBoxPlots(myLabelString,rowLabels,colGroups,dataMat,scale,fontsize):\n",
    "    plt.rcParams['pdf.fonttype']=42\n",
    "    # plt.rcParams['pdf.useafm'] = True\n",
    "    plt.rcParams['pdf.use14corefonts'] = True\n",
    "    plt.rcParams['text.usetex'] = False\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.rcParams.update({'font.weight': 'bold'})\n",
    "    plt.rcParams['axes.linewidth'] = 2 # set the value globally\n",
    "    for i,cname in enumerate(rowLabels):\n",
    "        myVals = []\n",
    "        for j,mygroup in enumerate(colGroups):\n",
    "            myVals.append((mygroup, dataMat[i,j]+1))\n",
    "        myVals = sorted(myVals, key=lambda x: x[0]) \n",
    "\n",
    "        data_to_plot = []\n",
    "        groupName = []\n",
    "        for key, group in groupby(myVals, lambda x: x[0]):\n",
    "            L = list(zip(*group)[1])\n",
    "            data_to_plot.append(L)\n",
    "            groupName.append(key)\n",
    "\n",
    "        fig = plt.figure(1, figsize=(18, 18))\n",
    "        ax = fig.add_subplot(111,yscale=scale)\n",
    "        bp = ax.boxplot(data_to_plot)\n",
    "        ax.set_xticklabels(groupName,rotation=40, ha='right',fontsize=fontsize)\n",
    "        ax.set_title(cname)\n",
    "        ax.grid('on',alpha=0.5)\n",
    "        ax.set_ylabel('Peak Area')\n",
    "        plt.tight_layout()\n",
    "        fig.savefig('data/%s/Boxplots/Boxplot_%s %s%s' % (myLabelString,myLabelString,re.sub('[^A-Za-z0-9]+', '', cname),'.pdf'))\n",
    "        fig.clear()\n",
    "    # plt.rcParams['ps.fonttype']=42\n",
    "    # plt.rcParams['ps.useafm']= True\n",
    "\n",
    "\n",
    "def saveData(myFolder,ticData,dictId,myExperimentID,dictData,data,fileInfo,export_fileIds):\n",
    "    metatlas_data = {'myLabelString':myFolder,'ticData':ticData,'dictId':dictId,'myExperimentID':myExperimentID,'dictData':dictData, 'data':data,'fileInfo':fileInfo,'export_fileIds':export_fileIds}\n",
    "    pickle.dump( metatlas_data, open( 'data/%s/dataset_%s.pkl' % (myFolder,myFolder), \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
